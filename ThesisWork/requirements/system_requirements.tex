
\chapter{Problem definition}

As discussed in the background section the U-Sem paradigm defines how the user modelling services has to be organized by orchestrating specific types of functional components. The specification, however, does not provide any formal mechanism or guidelines about how this services and components are constructed and managed in practice. Currently, the only work done in that direction is the adoption of a workflow management system (RDF Gears) which facilitates the orchestration of components into user modelling services. 

Because of the lack of standardized approach for building and managing all these components every engineer is responsible to adopt his own approach. Additionally, discovering the potential of Social media for user modelling \cite{brusilovsky2007adaptive}, has lead to an increased demand for U-Sem engineers to design and build many user modelling services. As a result engineers are constantly required to implement or adapt variety of functional components that are needed for the new services. We believe that the lack of formal approach and tools that support the process results in a lot of overhead work for engineers and can also potentially lead to increased amount of implementation errors. Therefore, in this work we investigate whether the current situation can be improved by introducing a user modelling server that is designed to support the process of creating and exposing the user modelling services to the users so that the overhead work of the engineers is limited and they can focus on the core of their work - the user modelling and analysis algorithms.

In order to be able to provide such system we, first, have to identify how engineers work and what are the tasks that require them to do a lot of overhead work and can be fully or partially automated. Next sections describe the approach that we used in order to identify the high level features that engineers can benefit most from.

\section{Requirements gathering}

In this section we aim to elicit the needs of the users and structure them into high-level system features that will later be designed and implemented.

\subsection{Interviews}

Stakeholders are the people that are have some kind interest in the project. They are the ones that will be affected by the project and thus they are the source for identifying the characteristics of the system we have to build. In our case the main stakeholders are the engineers that are already using or will use the U-Sem approach for constructing user modelling services.

Having identified the stakeholders of the system, literature suggests a wide variety of possible approaches for eliciting the requirements \cite{hickey2004unified} : interviews, questionnaires, user observation, workshops, brain storming, role playing, etc. 
At the end, we decided to use the semi-structured interviews technique since it is widely used and it has proved to be effective through the years \cite{dieste2008understanding}. 

\subsection{Identified features}

We analysed carefully all the raw information that was gathered from the interviews and we identified several high-level features. We presented them to the stakeholders and after some discussions we ended up with the following final list of features that the system should provide: 

\begin{itemize}

\item \textbf{Simplified access to social media}
Engineers base a lot of their analysis on the information provided by users in the social media. However, each social media provides access through a special API. Currently, each engineers has to implement a component for retrieving the required data from each type of social media. Providing feature that makes available to all scientists flexible components that can be configured to retrieve specific data from the different types of social media is considered a benefit from the engineers because it will save them considerable time and efforts. They also will not need to know the specific details concerning each of the APIs. 

\item \textbf{Plug-in environment}
Engineers constantly build new services and improve and adapt existing one. Therefore, engineers will benefit if the system is able to facilitate the process of extending itself by plugging in custom logic such as RDFGears functions and other functional components. Engineers should be able to manage(add/update/remove) this custom logic at runtime (without restarting the system). This process should not affect the work of other users.

\item \textbf{Universal data storage} 
Many of the engineers build services that need to store various types of data(e.g. intermediate and final results). Currently, they are forced to manually set-up databases and program the components needed for interacting with each type of data. This is usually not a trivial task, it requires time and knowledge and therefore, it resembles a big overhead to the engineers. Thus, they can be significantly benefited if the system provides a mechanism that enables storage and retrieval of arbitrary types of data.

 
\item \textbf{Integration with Hadoop}
The amount of information that has to be processed in the system can sometimes be huge. Therefore, sometimes, it has to be processed by external Hadoop based systems. Currently, engineers have to manually manage the exchange the information to and from the Hadoop systems. Therefore, providing a mechanism that facilitates and standardises the process is also considered as a benefit for some engineers.

\end{itemize}

\section{Feature prioritization}

Because of the limited scope of this thesis work we are focusing on the features that will bring the most advantage for the engineers. In order to do that, we have to prioritize the requirements based on the impact they will provide. This process, however, is not trivial since each of the stakeholders has slightly different view on the benefits provided by each of the features that we already identified. Therefore, we performed a research in order to find an approach that can be applied in this situation.

Requirements prioritization is a relatively old research topic and there are numerous approaches that are currently available \cite{moisiadis2002fundamentals}. The most popular include Quality Function Deployment (QFD), the Analytical Hierarchy Process(AHP), the cost-value approach proposed by Karlsson, Wiegers' method, as well as a variety of industrial practices such as team voting, etc. However, literature also suggests that there is no perfect solution for this problem and the applicability of each approach depends heavily on the particular situation it is used.

For our project, we choose the analytic hierarchy process approach \cite{roper1990analytic} . This decision was based on the fact that it is especially suitable for prioritizing a small number of requirements\cite{karlsson1997cost} and it is a proven and widely used \cite{karlsson1998evaluation}. In literature, as a disadvantage of this approach is considered the fact that it takes no account of interdependencies between requirements \cite{roper1990analytic}. However, this issue is not a problem for our solution because the project high-level features are loosely coupled and they have little dependency between each other. 


\subsection{Features prioritization using AHP}
In the Requirements gathering section we identified 5 high-level requirements(features) that cover the main functionality of the system. In this step we are using the AHP's pairwise comparison method in order to assess the relative value of the candidate requirements. We asked a group of four experienced project members to represent customers views. We instructed them on the process and asked them to perform  pairwise comparisons of the candidate requirements based on their value(importance). \textbf{Apendix 1} shows the form that they were asked to fill.

We let the participants to work alone, defining their own pace. We also allowed them to choose the order of the pair's comparison. Discussions were also allowed. When all participant finished the pairwise comparison, as advised in \cite{karlsson1997cost} we had to make sure that the provided results are consistent. In order to do that we had to calculate the the consistency indices of the pairwise comparisons. According to \cite{karlsson1997cost} values lower than 0.10 are considered acceptable and even values around .12 are commonly achieved in the industry and can also be considered acceptable. The calculation showed that two of the participants have indices higher than .23 which indicates serious inconsistencies (table \ref{tbl:reqInitialConsist}). Therefore, we asked them to revise their answers and the results afterwards we measured values that are acceptable (table \ref{tbl:reqFinalConsist}).
 
 \begin{table}[h!]
  \begin{center}
    \begin{tabular}{| l | l | l | l | l |}
    \hline
    & Stakeholder 1 & Stakeholder 2 & Stakeholder 3 & Stakeholder 4 \\	 \hline
    Consistency ratio & 0.04 & 0.23 & 0.13 & 0.26 \\
    \hline
    \end{tabular}
  \end{center}
  \caption{The initial consistency ratios for each of the stakeholders.}
  \label{tbl:reqInitialConsist}
\end{table}


 \begin{table}[h!]
  \begin{center}
    \begin{tabular}{| l | l | l | l | l |}
    \hline
    & Stakeholder 1 & Stakeholder 2 & Stakeholder 3 & Stakeholder 4 \\	 \hline
    Consistency ratio & 0.04 & 0.12 & 0.13 & 0.11 \\
    \hline
    \end{tabular}
  \end{center}
  \caption{Consistency ratios for each of the stakeholders after refinement.}
  \label{tbl:reqFinalConsist}
\end{table}

Once we had achieved satisfying results we calculated the distributions. We outlined the candidate requirements in a diagram (figure \ref{fig:reqPriority}) and presented the results to the project members. Each requirement's determined value is relative and based on a ratio scale. Therefore, a requirement whose value is calculated as 0.20 is twice as valuable as a requirement with a value of 0.10. Additionally, the sum of the values for all requirements equals 1. This means that a requirement with a value of 0.10 provides 10 percent of the value of all the requirements.

\begin{figure}[h!]
  \centering
      \includegraphics{requirements/value_diagram.png}
  \caption{The value distribution of the 5 requirements in the U-Sem project.}
  \label{fig:reqPriority}
\end{figure}


Finally, based on the provided results from the prioritization and the limited scope of this thesis we decided to further address the two features that will provide the most benefits for the users: "Plug-in Environment" and "Data Management". Next section sections cover each of the problems in details.

\section{Plug-in Environment}
\label{sec:problemDefPlugin}

During the initial interviews with the engineers that are going to potentially use the system, we revealed that the nature of their work is very dynamic. In their day to day work they are expected to constantly improve and come up with new algorithms and approaches for user modelling. As a result, they are continuously producing new software code that implements these algorithms. After each production cycle, the program code has to be deployed into U-Sem so that for the purpose of experimentation, it is available for testing, demonstration and evaluation purposes.

We also performed additional interviews with the engineers in order to reveal how this process is currently done, what are the problems they currently face and cost them a time and efforts, and what are their expectations for the future system. Based on the results from the interviews we concluded that the main problem is that engineers have to "hardcode" the code implementing new components into the source code of the workflow engine (RDF Gears). In this way, the software code implementing the algorithms becomes part of the workflow engine. This approach has a lot of disadvantages, it is error prone and brings a lot of discomfort to the engineers working with the system because:

\begin{itemize}

	\item Adding new or modifying existing functionality requires a lot of time and knowledge since in order to do that one has to alter the source code of the workflow engine and basically, release a new version of it. This process requires advanced knowledge about each phase of the release process: checking out the source code from the software repository, putting the new source code in the appropriate place, building the system and finally, deploying it to the web server. Most of the time, all this knowledge is not required for the daily work of scientists and learning it may create a serious overhead and discomfort.
	
	\item In order to add/modify functionality one has to stop the web server where the system is deployed, replace the deployment entities of the system and start the server again. The problem with this approach is that during the time the server is down all other running services are unavailable. This is a major problem for everyone that is using the system during that time.
	
	\item Another major disadvantage is that, as a result of all the additional knowledge required, the training period for new scientists is significantly increased. This may easily cause project delays and missed deadlines.
	
	\item Multiple scientists adding/modifying functionality simultaneously may result in loss of functionality. Figure \ref{fig_vers_prob} illustrates the problematic scenario. As stated earlier, in order to add new functionality, scientists must first check out the source code of the system, make the changes and deploy the new version on the web server. However, if two scientist perform this process simultaneously then the new functionality provided by the first scientist will be lost when the second one deploys his version. 
	
	\begin{figure}[h!]
  \centering
  	\includegraphics[scale=0.75]{plug-in/version_problem.png}
  \caption{State diagram illustrating the scenario where two scientists extend U-Sem simultaneously and the changes made by Scientist A are lost.  }
  \label{fig_vers_prob}
\end{figure}
	
	\item And last but not least, it is hard to verify what is the exact state of the system at any particular moment. Unless documented exclusively, it is not clear what additional functionality is added to the system. This problem becomes more serious when there are more people working on the project simultaneously and it is hard to track the changes in the system.
		
\end{itemize}

This approach also introduces one disadvantage from software engineering perspective and it concerns the lack of a mechanism for establishing modularization between the implemented components. In software engineering, modularization is considered as a key property for improving extensibility, comprehensibility, and reusability in software projects \cite{parnas1972criteria}. The most important aspect of a successful modular system is its information hiding capabilities \cite{srivastava2008modular}. In our case, scientists can only rely on the modularization functionality provided by the Java language. However, its information hiding principles are only applied on class level, but not to the level of packages and JAR files. For example, it is not possible to restrict access to certain public classes defined in a package. The absence of such visibility control can easily lead to highly coupled, "spaghetti-like" systems \cite{eder1994coupling}. The consequences of this will become more and more clear with the time when the system grows in size, complexity and the number of engineers working on it increases. The most probable consequences include high development costs, low productivity, unmanageable software quality and high risk to move to new technology \cite{cai2000component}.

\textbf{TODO Transition to the next part of the story}

\section{Data Management}
\label{sec:problemDefStorage}

Our research showed that many of the services implemented by the engineers require data storage and retrieval in some form. The structure and semantics of the data varies greatly but it can be roughly classified in the following groups:

\begin{itemize}

	\item \textit{Raw data} - Engineers reported that most of the analysis and modelling services they devised are based on the social media. Basically, they have to retrieve certain entries(e.g tweets from Twitter ) which are the basis for the analysis. In certain cases engineers have to store this "raw data" locally. Usually, this is as a result of the fact that the retrieving the entities requires time and also some social media APIs limit the number of entities you can get in certain amount of time \cite{cheong2009integrating}. This makes the execution of workflows slow(the system has to wait to get the needed entries every time). Therefore, engineers are forced to store these entries locally and only make sure they are up-to-date prior to the execution of a workflow.
	
	\item \textit{User provided data} - Some services require information that is not available from the social media and has to be provided by the users of the system. For example, in certain usecases users has to fill in questioners and the results from later analysis is adjusted based on the answers provided by the users. It is infeasible to ask the users for that information every time that is why it has to be stored within the system.
	
	\item \textit{Intermediate results} - Some services consist of two phases. The first phase continuously calculates some intermediate representation of the raw data. In the second phase, on user request the final result is calculated based on the intermediate data. One of the examples is the Twinder service \cite{tao2012twinder}. Therefore, between the two phases the intermediate results have to be stored and later retrieved back.
	
	\item \textit{Ready user profiles} - Some services require that users has to be able to monitor how the results evolve over time. For example, in e-learning systems users want to be able to see how the knowledge of a certain person has changed after following a certain course in order to measure how helpful the course was for that person. Therefore, every time a service is executed the results have to be stored so that they can be later further analysed.
	
	\item \textit{System data} - Finally, many of the features of the system need to store some kind of information. For example, the multy-user sport has to store all kind information about the users of the system: user names, passwords, privileges, etc. The scheduling feature has to store information about the time each workflow has to be executed. 
	
\end{itemize}

Currently, the system does not provide any functionality to support users in defining workflows that require data operations. Each scientist is forced to create custom components that serve the particular requirements. However, this process costs a lot of time and efforts, and suffers from many downsides and problems:

\begin{itemize}
	\item \textit{Knowledge required} - designing and implementing components for dealing with persistent data is not a trivial job. It requires specific type of knowledge(\textbf{database, administration}). Many of the engineers building services have mathematical or statistical backgrounds and are likely not to have in depth knowledge in the database field. In order to be able to build their services they have to acquire this knowledge which can cause significant overhead and waste of time. Additionally, the fact that they are not professionals in the field may lead to problems and shortcomings.
	
	\item \textit{Server administration} - Most of the storing solutions require setting up a dedicated database server. These servers have to be hosted somewhere, maintained, backuped, etc. All these require a lot of effort and if every user has to do it, it will result in large amount of duplicated work and overhead. If engineers decide to use a shared database server then appears the question of who is responsible to manage it and ensure its security and privacy.
	
	\item \textit{Dynamic data structure} - It is expected that the structure of the stored data might change over time. When a database with fixed schema is used (like most SQL solutions) then every time the structure changes the engineers has to manually connect to the database and apply the changes manually. This can be really annoying, time consuming and error prone. Therefore, automating this process can save time to engineers and reduce the number of mistakes caused by carelessness. 
	
	\item \textit{Collaboration} - collaborations between engineers on data level is reported to be quite important and can save them a lot of time. Currently, there are no facilities available to support that requirement. Engineers have to organize this collaborations personally. Additionally, because the collaboration are not integrated in the system they are likely to be hard to monitor and control.
	
	\item \textit{Low level queries} - Currently, most services use SQL databases. As a result, engineers have to build various SQL queries. The SQL query language is considered as low level language \textbf{needs ref} and causes some overhead to the engineers especially the ones that are not familiar with it. Therefore, introducing higher level query language can be a big plus.
	
	\item \textit{RGL translation} - Generally available database solutions are not capable to deal with data in the RGL format introduced in RDF Gears. Therefore, every single component that deals with persistent data has to translate the RGL values to values compatible with the databases solution and vice versa. Clearly, all this code is redundant and removing this responsibility from the engineers will save them time and efforts so that they can focus their attention to the core of their work.
	
	\item \textit{RDF Gears and components with side effects} - Components that store data are components that have side affects. However, RDF Gears is not designed to work with such components and some unexpected behaviour might be expected. Therefore, engineers building components with side effects and are not aware of the way RDF Gears operate internally risk to introduce problems that are hard to detect.
	
\end{itemize}

In this thesis we aim to propose a solution that is capable to overcome these problems and save engineers a lot of time, efforts and prevent mistakes while dealing with persistent data.

\section{Conclusion}

\textbf{TODO}